{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32fjpkeS-nYP"
      },
      "source": [
        "PySpark is Python interface for Apache Spark. The primary use cases for PySpark are to work with huge amounts of data and for creating data pipelines.\n",
        "\n",
        "You don't need to work with big data to benefit from PySpark. SparkSQL is a great tool for performing routine data anlysis. Pandas can get slow and you may find yourself writing a lot of code for data cleaning whereas the same actions take much less code in SQL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9yXqV3LigUA"
      },
      "source": [
        "# 1. Installing PySpark in Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "g8yHLyY5AoOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Download `spark-3.2.1` from https://dlcdn.apache.org/spark"
      ],
      "metadata": {
        "id": "iRiOvrXGA4Vo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' Add your code here '''"
      ],
      "metadata": {
        "id": "gQ7lRCr2Am0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Uncompress spark"
      ],
      "metadata": {
        "id": "PPN8KHLAA9XC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' Add your code here '''"
      ],
      "metadata": {
        "id": "y5PklydyAtTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Imports"
      ],
      "metadata": {
        "id": "2jRx6XpfBUW9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxv7w_2y2bb9"
      },
      "source": [
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import findspark\n",
        "\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Start a spark Session"
      ],
      "metadata": {
        "id": "JBsrXIafBZkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Create a spark Session\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"Spark Example\") \\\n",
        "       .getOrCreate()"
      ],
      "metadata": {
        "id": "6wb96qZpBPr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcOCBgQo2Pqf"
      },
      "source": [
        "spark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE4zy4gdbLnE"
      },
      "source": [
        "# Setup - 1 : Creating a Test Spark DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "750m7yL29U4g"
      },
      "source": [
        "## Consider this sample dataset with details of \"first_name\", \"last_name\" & \"user_id\"\n",
        "data = [\n",
        "        ('John','Smith',1),\n",
        "        ('Jane','Smith',2),\n",
        "        ('Jonas','Smith',3),\n",
        "]\n",
        "\n",
        "\n",
        "## Create a spark dataframe out of above data\n",
        "df = ''' Add your code here '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17sJ9jaliV-b"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP1GBfIoxIKw"
      },
      "source": [
        "## Show first 2 rows of the dataframe\n",
        "''' Add your code here '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTvI4jbZjX31"
      },
      "source": [
        "# Steup - 2 : Example with a Pre-existing Data Set\n",
        "This data set comes with your Google Colab Session - `sample_data/california_housing_train.csv`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-To1oW2S4mZL"
      },
      "source": [
        "## Read the dataset using \"spark\"\n",
        "df = ''' Add your code here '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvDrXp8w4pFi"
      },
      "source": [
        "## Print schema for the read dataframe\n",
        "''' Add your code here '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-ra4P9Z7sut"
      },
      "source": [
        "## Print first 10 rows\n",
        "''' Add your code here '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKzuJaaw79Kf"
      },
      "source": [
        "## Get the number of entries in the dataframe\n",
        "''' Add your code here '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvjbab-J7_t_"
      },
      "source": [
        "## Print median age & total rooms for first 5 records\n",
        "''' Add your code here '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yl85UrLC8PYW"
      },
      "source": [
        "## Get distinct values from the column \"total_rooms\"\n",
        "''' Add your code here '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5tOUkPJ8XRb"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "## Get sum of 'housing_median_age' for each group of 'total_rooms'\n",
        "test = ''' Add your code here '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv6f3pGb_XR8"
      },
      "source": [
        "test.toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHvKMqLQ4ezk"
      },
      "source": [
        "# Setup -3 : Reading Data from external source\n",
        "\n",
        "For this example, We are going to use a publicly available data set in a CSV format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzvNxiQSixRU"
      },
      "source": [
        "import requests\n",
        "path = \"https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv\"\n",
        "req = requests.get(path)\n",
        "url_content = req.content\n",
        "\n",
        "csv_file_name = 'owid-covid-data.csv'\n",
        "csv_file = open(csv_file_name, 'wb')\n",
        "\n",
        "csv_file.write(url_content)\n",
        "csv_file.close()\n",
        "\n",
        "### Read the downloaded file using spark into a dataframe ###\n",
        "df = ''' Add your code here '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYRUC46L_8zX"
      },
      "source": [
        "## 3.1 PySpark DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-PgzP3IjZsV"
      },
      "source": [
        "#Viewing the dataframe schema\n",
        "''' Add your code here '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KKBv0ZCFbP5"
      },
      "source": [
        "#Converting a date column\n",
        "df.select(F.to_date(df.date).alias('date'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2ylA4B2kfd2"
      },
      "source": [
        "#Summary stats\n",
        "''' Add your code here '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRX6qF_dEp9l"
      },
      "source": [
        "#DataFrame Filtering\n",
        "\n",
        "# Find newest cases from \"United States\"\n",
        "''' Add your code here '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt6e41cFEFSR"
      },
      "source": [
        "# 4. Spark SQL\n",
        "\n",
        "There is less to learn since it's basically the same SQL syntax you might already be comfortable with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBpoPIGDrb-c"
      },
      "source": [
        "#Creating a table from the dataframe\n",
        "## Make sure your data is in a dataframe called \"df\"\n",
        "\n",
        "df.createOrReplaceTempView(\"covid_data\") #temporary view"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFcoi5l7kyLq"
      },
      "source": [
        "# Write sql query to print all data from the temporary view created above.\n",
        "df2 = ''' Add your code here '''\n",
        "df2.printSchema()\n",
        "df2.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teHD2Up4k4Cd"
      },
      "source": [
        "# Write sql query to get count from each location\n",
        "groupDF = ''' Add your code here '''\n",
        "groupDF.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}