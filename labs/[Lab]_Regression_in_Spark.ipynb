{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxra6A6DgXK6dxG0kSLykW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namitakalra-google/Big-Data-Workshop/blob/main/labs/%5BLab%5D_Regression_in_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PySpark Machine Learning Example to Implement Linear Regression"
      ],
      "metadata": {
        "id": "1X4ag7GW9MTo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9yXqV3LigUA"
      },
      "source": [
        "# 1. Installing PySpark in Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "g8yHLyY5AoOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Download `spark-3.5.1` from https://dlcdn.apache.org/spark"
      ],
      "metadata": {
        "id": "iRiOvrXGA4Vo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' your code here '''"
      ],
      "metadata": {
        "id": "gQ7lRCr2Am0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Uncompress spark"
      ],
      "metadata": {
        "id": "PPN8KHLAA9XC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' your code here '''"
      ],
      "metadata": {
        "id": "y5PklydyAtTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Imports"
      ],
      "metadata": {
        "id": "2jRx6XpfBUW9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxv7w_2y2bb9"
      },
      "source": [
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.ml.stat import Correlation\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Start a spark Session"
      ],
      "metadata": {
        "id": "JBsrXIafBZkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Create a spark Session\n",
        "spark= ''' your code here '''"
      ],
      "metadata": {
        "id": "6wb96qZpBPr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcOCBgQo2Pqf"
      },
      "source": [
        "spark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Get data & upload to Google Colab\n",
        "\n",
        "https://github.com/namitakalra-google/Big-Data-Workshop/blob/c29dfe7cea49c91b5beea601988fa1c33da0b94b/insurance_1.csv\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "1ql2bsbP_Ox-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the uploaded file using spark into a dataframe"
      ],
      "metadata": {
        "id": "RsnX-nHl_5FH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = ''' your code here '''"
      ],
      "metadata": {
        "id": "9Ga_pUIl_moG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Exploration for better understanding via data samples -\n",
        "\n",
        "Our data has 7 columns where the 'expenses' column is our dependent variable and 'age', 'gender', 'BMI', 'children', 'smoker', and 'region' are the independent variables."
      ],
      "metadata": {
        "id": "1_RUG0-3_6wG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(2)"
      ],
      "metadata": {
        "id": "pPs8o888__3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Feature Engineering"
      ],
      "metadata": {
        "id": "Xqng8I4OA34K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Use StringIndexer to deal with `Categorical` variables"
      ],
      "metadata": {
        "id": "K9ADGnKkBRFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**StringIndexer**: It is used to convert a string column into numerical form. It allocates unique values to each of the categories present in the respective column."
      ],
      "metadata": {
        "id": "_tiGR73oBbGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer"
      ],
      "metadata": {
        "id": "1PwIBrMnAHvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexer = StringIndexer(inputCol = \"gender\", outputCol = \"gender_cat\")\n",
        "indexed = indexer.fit(df).transform(df)"
      ],
      "metadata": {
        "id": "iy4z7-ONBgiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do the same for other two categorial columns in the dataset!"
      ],
      "metadata": {
        "id": "7vhipiH7JfIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' your code here '''"
      ],
      "metadata": {
        "id": "Xgreg3L1CoSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' your code here '''"
      ],
      "metadata": {
        "id": "i3HyXmK8CwGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Merge multiple columns into a single column using `VectorAssembler`\n",
        "\n",
        "**VectorAssembler**: It is a transformer that helps to select the input columns that we need to create a single feature vector to train our Machine Learning models."
      ],
      "metadata": {
        "id": "94y93ZBhC89n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.linalg import Vector"
      ],
      "metadata": {
        "id": "oAaoJvoBC6gQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identify what cols will become your training inputs, create an `assembler` to club them to a single column called `features` using `VectorAssembler`"
      ],
      "metadata": {
        "id": "pYDNV0CRJn9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assembler = ''' your code here '''"
      ],
      "metadata": {
        "id": "0mrmKEbqDH1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assembler"
      ],
      "metadata": {
        "id": "a2VVYNuaDmKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you actually do the clubbing -"
      ],
      "metadata": {
        "id": "oCfIasTuJ-ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "#change our features into one column using our defined assembler\n",
        "trainingDataFinal = assembler.transform(indexed).select(\n",
        "    col(\"features\"), (col(\"expenses\").cast(\"Float\").alias(\"label\")))\n",
        "trainingDataFinal.show(truncate=False , n=3)"
      ],
      "metadata": {
        "id": "c91iogFiDqXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Splitting the Dataset\n",
        "\n",
        "Splitting the Data into Train and Test sets to train our model and check its efficiency.\n",
        "\n",
        "Make a `70 - 30` distribution for training & test data"
      ],
      "metadata": {
        "id": "XITknNpyE9fU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = ''' your code here '''"
      ],
      "metadata": {
        "id": "ApIus4jSFA6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking the statistics of our train and test sets."
      ],
      "metadata": {
        "id": "fCROF8flFXBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.describe().show()"
      ],
      "metadata": {
        "id": "RP30C9h0FPVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.describe().show()"
      ],
      "metadata": {
        "id": "pT3BXXe_Faip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Build and Train Linear Regression Model\n",
        "\n",
        "Linear regression is all about finding patterns in data and using those patterns to make predictions about new situations."
      ],
      "metadata": {
        "id": "ODa5eZm2FhiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import LinearRegression"
      ],
      "metadata": {
        "id": "tzvOvDH6FdNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a Linear Regression Model using the `LinearRegression` imported above"
      ],
      "metadata": {
        "id": "a9PoyYgsKShP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#call Spark linear regression we import before\n",
        "model = ''' your code here '''"
      ],
      "metadata": {
        "id": "zTOuA-AlGL-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train the model\n",
        "trained_model = ''' your code here '''\n",
        "print (\"Regression model is trained!\")"
      ],
      "metadata": {
        "id": "UyANSIvPGV-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at the error values!"
      ],
      "metadata": {
        "id": "BKaehkPnGwiY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate your model on the `train_data` itself."
      ],
      "metadata": {
        "id": "GcV-jM26Keje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_results = ''' your code here '''"
      ],
      "metadata": {
        "id": "tgpVzd6CGh42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print `Mean Squared Error` for your results"
      ],
      "metadata": {
        "id": "fIbQZE7HKlmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' your code here '''"
      ],
      "metadata": {
        "id": "U_zIXK9qG87O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Evaluating Test Data"
      ],
      "metadata": {
        "id": "BxWpJfavHPte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#predict testing data using our model\n",
        "prediction = ''' your code here '''\n",
        "\n",
        "#show some prediction results\n",
        "prediction.show(3)"
      ],
      "metadata": {
        "id": "-gE0sJz0HA_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Calculate our model performance"
      ],
      "metadata": {
        "id": "2li-n8UHHuPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import evaluator module for regression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator"
      ],
      "metadata": {
        "id": "IYnSkqFjHmlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define our evaluator\n",
        "evaluator = RegressionEvaluator(\n",
        "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")"
      ],
      "metadata": {
        "id": "VuwmE60nK4WY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate RMSE of our trained model\n",
        "rmse = ''' your code here '''\n",
        "print (\"Root Mean Square Error (RMSE):\", rmse)"
      ],
      "metadata": {
        "id": "5sSleK4VH3rF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9HrzZySxIA0q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}