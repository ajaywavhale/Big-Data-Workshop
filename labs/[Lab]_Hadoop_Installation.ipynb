{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "lb_WWiB-QfRI",
        "b7dNs50tQgse",
        "IxtOKT-fQ1zU"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Hadoop\n",
        "Hadoop is a **Java-based** programming framework that supports the processing and storage of extremely large datasets on a cluster of inexpensive machines. It was the first major open source project in the big data playing field and is sponsored by the Apache Software Foundation."
      ],
      "metadata": {
        "id": "IMnjM5yjIXmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Verifying JAVA Installation\n",
        "Java must be installed on your system before installing Hadoop. Let us verify java installation using the following command:\n",
        "\n",
        "`!java --version`"
      ],
      "metadata": {
        "id": "dv1x-wgqG6n0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!java --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "el_lCHn1G-Ph",
        "outputId": "82718a4b-0075-47b6-af27-dc42395e7e6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk 11.0.22 2024-01-16\n",
            "OpenJDK Runtime Environment (build 11.0.22+7-post-Ubuntu-0ubuntu222.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.22+7-post-Ubuntu-0ubuntu222.04.1, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Use `wget` command to download latest Hadoop version from Apache's website\n",
        "https://downloads.apache.org/hadoop/common"
      ],
      "metadata": {
        "id": "_PbUe_I1HgjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://downloads.apache.org/hadoop/common/hadoop-3.4.0/hadoop-3.4.0.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgcl8t7qHIJ5",
        "outputId": "cfec8c10-88e9-4a6a-e504-a12b43409549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-07 20:36:07--  https://downloads.apache.org/hadoop/common/hadoop-3.4.0/hadoop-3.4.0.tar.gz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 88.99.208.237, 135.181.214.104, 2a01:4f9:3a:2c57::2, ...\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|88.99.208.237|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 965537117 (921M) [application/x-gzip]\n",
            "Saving to: ‘hadoop-3.4.0.tar.gz’\n",
            "\n",
            "hadoop-3.4.0.tar.gz 100%[===================>] 920.81M  19.7MB/s    in 47s     \n",
            "\n",
            "2024-04-07 20:36:55 (19.4 MB/s) - ‘hadoop-3.4.0.tar.gz’ saved [965537117/965537117]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Extract the downloaded Hadoop package using the `!tar -xzvf` command.\n",
        "\n",
        "* -x flag to extract,\n",
        "* -z to uncompress,\n",
        "* -v for verbose output, and\n",
        "* -f to specify that we’re extracting from a file"
      ],
      "metadata": {
        "id": "QUsBN3qhH33O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzvf hadoop-3.4.0.tar.gz"
      ],
      "metadata": {
        "id": "qMLtyQs3ILva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Copy  hadoop file to `/user/local`"
      ],
      "metadata": {
        "id": "kBCnP133KHyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r hadoop-3.4.0/ /usr/local/"
      ],
      "metadata": {
        "id": "_HMRTYmtKpbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Set environment variable `HADOOP_HOME` to `/user/local/<hadoop_version/>`"
      ],
      "metadata": {
        "id": "grn0h95pPHiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop-3.4.0\""
      ],
      "metadata": {
        "id": "xTiDT2fpPWW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Configuring Hadoop’s Java Home\n",
        "Hadoop requires that you set the path to Java, either as an environment variable or in the Hadoop configuration file."
      ],
      "metadata": {
        "id": "ywRDBwaZK34J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!readlink -f /usr/bin/java | sed \"s:bin/java::\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQFz-CP-K7AV",
        "outputId": "7f1999fe-22f3-4362-82b5-baa7a3e93a6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/jvm/java-11-openjdk-amd64/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Set as environment variable"
      ],
      "metadata": {
        "id": "5DHv7jlzLJrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64/\""
      ],
      "metadata": {
        "id": "4SHyXvpdLNoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Set in Hadoop configuration file\n",
        "\n",
        "set in /usr/local/hadoop-3.4.0/etc/hadoop/hadoop-env.sh\n",
        "`export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/`"
      ],
      "metadata": {
        "id": "fxie0hNBLZgB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Running Hadoop"
      ],
      "metadata": {
        "id": "aMftTasBL_gW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.4.0/bin/hadoop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uY2CzxKFLwPm",
        "outputId": "12639d71-5ced-4553-ba24-74235fe7c4f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: hadoop [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\n",
            " or    hadoop [OPTIONS] CLASSNAME [CLASSNAME OPTIONS]\n",
            "  where CLASSNAME is a user-provided Java class\n",
            "\n",
            "  OPTIONS is none or any of:\n",
            "\n",
            "buildpaths                       attempt to add class files from build tree\n",
            "--config dir                     Hadoop config directory\n",
            "--debug                          turn on shell script debug mode\n",
            "--help                           usage information\n",
            "hostnames list[,of,host,names]   hosts to use in worker mode\n",
            "hosts filename                   list of hosts to use in worker mode\n",
            "loglevel level                   set the log4j level for this command\n",
            "workers                          turn on worker mode\n",
            "\n",
            "  SUBCOMMAND is one of:\n",
            "\n",
            "\n",
            "    Admin Commands:\n",
            "\n",
            "daemonlog     get/set the log level for each daemon\n",
            "\n",
            "    Client Commands:\n",
            "\n",
            "archive       create a Hadoop archive\n",
            "checknative   check native Hadoop and compression libraries availability\n",
            "classpath     prints the class path needed to get the Hadoop jar and the required libraries\n",
            "conftest      validate configuration XML files\n",
            "credential    interact with credential providers\n",
            "distch        distributed metadata changer\n",
            "distcp        copy file or directories recursively\n",
            "dtutil        operations related to delegation tokens\n",
            "envvars       display computed Hadoop environment variables\n",
            "fedbalance    balance data between sub-clusters\n",
            "fs            run a generic filesystem user client\n",
            "gridmix       submit a mix of synthetic job, modeling a profiled from production load\n",
            "jar <jar>     run a jar file. NOTE: please use \"yarn jar\" to launch YARN applications, not this\n",
            "              command.\n",
            "jnipath       prints the java.library.path\n",
            "kdiag         Diagnose Kerberos Problems\n",
            "kerbname      show auth_to_local principal conversion\n",
            "key           manage keys via the KeyProvider\n",
            "rbfbalance    move directories and files across router-based federation namespaces\n",
            "rumenfolder   scale a rumen input trace\n",
            "rumentrace    convert logs into a rumen trace\n",
            "s3guard       S3 Commands\n",
            "version       print the version\n",
            "\n",
            "    Daemon Commands:\n",
            "\n",
            "kms           run KMS, the Key Management Server\n",
            "registrydns   run the registry DNS server\n",
            "\n",
            "SUBCOMMAND may print help when invoked w/o parameters or with -h.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Copy the config files"
      ],
      "metadata": {
        "id": "voa9X7-CMh3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/input"
      ],
      "metadata": {
        "id": "8e8SI4goS3QT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /usr/local/hadoop-3.4.0/etc/hadoop/*.xml ~/input"
      ],
      "metadata": {
        "id": "BdqlozkNMknW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Review the config files"
      ],
      "metadata": {
        "id": "xa-U2S0tMpnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ~/input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXqEa7L3Mvy9",
        "outputId": "14476d3b-8902-4e0d-f32d-1c9c38a6bfee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "capacity-scheduler.xml\thadoop-policy.xml  hdfs-site.xml    kms-acls.xml  mapred-site.xml\n",
            "core-site.xml\t\thdfs-rbf-site.xml  httpfs-site.xml  kms-site.xml  yarn-site.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given below are the list of files that you have to edit to configure Hadoop.\n",
        "\n",
        "1. core-site.xml\n",
        ": The core-site.xml file contains information such as the port number used for Hadoop instance, memory allocated for the file system, memory limit for storing the data, and the size of Read/Write buffers.\n",
        "\n",
        "2. hdfs-site.xml: The hdfs-site.xml file contains information such as the value of replication data, the namenode path, and the datanode path of your local file systems. It means the place where you want to store the Hadoop infra.\n",
        "\n",
        "3. yarn-site.xml: This file is used to configure yarn into Hadoop.\n",
        "\n",
        "4. mapred-site.xml: This file is used to specify which MapReduce framework we are using."
      ],
      "metadata": {
        "id": "QRro7SUxPqEq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Verifying Hadoop Installation"
      ],
      "metadata": {
        "id": "e4rbnNwlQT48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step I: Name Node Setup"
      ],
      "metadata": {
        "id": "lb_WWiB-QfRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.4.0/bin/hdfs namenode -format"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WluXgjBLQZ1P",
        "outputId": "35a24ca3-d377-4f90-9479-6a5b503b9d2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: /usr/local/hadoop-3.4.0/logs does not exist. Creating.\n",
            "2024-04-07 20:41:51,775 INFO namenode.NameNode: STARTUP_MSG: \n",
            "/************************************************************\n",
            "STARTUP_MSG: Starting NameNode\n",
            "STARTUP_MSG:   host = 606cd77ca520/172.28.0.12\n",
            "STARTUP_MSG:   args = [-format]\n",
            "STARTUP_MSG:   version = 3.4.0\n",
            "STARTUP_MSG:   classpath = /usr/local/hadoop-3.4.0/etc/hadoop:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/jetty-io-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/zookeeper-jute-3.8.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/jersey-server-1.19.4.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/kerby-pkix-2.0.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/jetty-security-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-smtp-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/jline-3.9.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/jul-to-slf4j-1.7.36.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/commons-io-2.14.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-handler-ssl-ocsp-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/jetty-http-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-xml-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-resolver-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/zookeeper-3.8.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/commons-lang3-3.12.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/kerb-client-2.0.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/jetty-servlet-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/jetty-util-ajax-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/dnsjava-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-mqtt-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-haproxy-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-buffer-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-classes-kqueue-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/jackson-core-2.12.7.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-redis-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/commons-codec-1.15.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/kerb-util-2.0.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/audience-annotations-0.12.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/jackson-annotations-2.12.7.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/avro-1.9.2.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/httpcore-4.4.13.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/commons-configuration2-2.8.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/slf4j-api-1.7.36.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/commons-text-1.10.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-stomp-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/bcprov-jdk15on-1.70.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/woodstox-core-5.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-common-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/curator-framework-5.2.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-http-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/guava-27.0-jre.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/kerby-util-2.0.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-udt-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/checker-qual-2.5.2.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/reload4j-1.2.22.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/hadoop-auth-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-native-unix-common-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/kerb-core-2.0.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/jersey-servlet-1.19.4.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/kerby-config-2.0.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/jsch-0.1.55.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.100.Final-osx-aarch_64.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-native-epoll-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/commons-cli-1.5.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/jackson-databind-2.12.7.1.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/jersey-json-1.20.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/curator-client-5.2.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/hadoop-shaded-guava-1.2.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/commons-math3-3.6.1.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/token-provider-2.0.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-resolver-dns-classes-macos-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-all-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/commons-net-3.9.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-native-epoll-4.1.100.Final-linux-aarch_64.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/httpclient-4.5.13.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-handler-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/jetty-xml-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/jsr305-3.0.2.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/jettison-1.5.4.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/kerb-server-2.0.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/metrics-core-3.2.4.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/jetty-server-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-resolver-dns-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/kerby-xdr-2.0.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-handler-proxy-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-http2-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/jetty-webapp-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/hadoop-shaded-protobuf_3_21-1.2.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-classes-epoll-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/kerb-admin-2.0.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-aarch_64.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/kerb-crypto-2.0.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-socks-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/jersey-core-1.19.4.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/hadoop-annotations-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/snappy-java-1.1.10.4.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/kerb-common-2.0.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-rxtx-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-native-epoll-4.1.100.Final-linux-x86_64.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/gson-2.9.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/jetty-util-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-memcache-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/kerby-asn1-2.0.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/commons-compress-1.24.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-dns-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/nimbus-jose-jwt-9.31.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/commons-logging-1.2.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-sctp-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/kerb-identity-2.0.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/kerb-simplekdc-2.0.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/stax2-api-4.2.1.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.100.Final-osx-x86_64.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-x86_64.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/failureaccess-1.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/curator-recipes-5.2.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/hadoop-kms-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/hadoop-registry-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/hadoop-common-3.4.0-tests.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/hadoop-nfs-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/common/hadoop-common-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-io-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/zookeeper-jute-3.8.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/jersey-server-1.19.4.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/kerby-pkix-2.0.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-security-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-smtp-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/jline-3.9.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-io-2.14.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-handler-ssl-ocsp-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-http-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-xml-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-resolver-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/zookeeper-3.8.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-lang3-3.12.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-client-2.0.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-servlet-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/dnsjava-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-mqtt-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-haproxy-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-buffer-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-classes-kqueue-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/jackson-core-2.12.7.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-redis-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-codec-1.15.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-util-2.0.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/audience-annotations-0.12.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/jackson-annotations-2.12.7.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/avro-1.9.2.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-configuration2-2.8.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-text-1.10.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-stomp-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/woodstox-core-5.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-common-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/curator-framework-5.2.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-http-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/kerby-util-2.0.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-udt-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/reload4j-1.2.22.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/hadoop-auth-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-native-unix-common-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-core-2.0.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.4.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/kerby-config-2.0.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.100.Final-osx-aarch_64.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-cli-1.5.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/jackson-databind-2.12.7.1.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/jersey-json-1.20.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/curator-client-5.2.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/hadoop-shaded-guava-1.2.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-math3-3.6.1.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/token-provider-2.0.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-resolver-dns-classes-macos-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-all-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-net-3.9.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.100.Final-linux-aarch_64.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-handler-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-xml-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/jettison-1.5.4.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-server-2.0.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/metrics-core-3.2.4.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-server-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-resolver-dns-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/kerby-xdr-2.0.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-handler-proxy-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-http2-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-webapp-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_21-1.2.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-classes-epoll-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-admin-2.0.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-aarch_64.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-crypto-2.0.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-socks-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/jersey-core-1.19.4.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/hadoop-annotations-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/snappy-java-1.1.10.4.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-common-2.0.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-rxtx-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.100.Final-linux-x86_64.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/HikariCP-4.0.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/gson-2.9.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-util-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-memcache-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/kerby-asn1-2.0.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-compress-1.24.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-dns-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.31.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-logging-1.2.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-sctp-4.1.100.Final.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-identity-2.0.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-simplekdc-2.0.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.100.Final-osx-x86_64.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-x86_64.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/lib/curator-recipes-5.2.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-3.4.0-tests.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-rbf-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-rbf-3.4.0-tests.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-client-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.4.0-tests.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-client-3.4.0-tests.jar:/usr/local/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.4.0-tests.jar:/usr/local/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/websocket-server-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/bcpkix-jdk15on-1.70.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/jersey-client-1.19.4.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/websocket-common-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/websocket-servlet-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/javax.websocket-api-1.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/websocket-api-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.12.7.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/objenesis-2.6.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/asm-tree-9.6.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/javax-websocket-server-impl-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/jna-5.2.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/jetty-plus-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/jetty-client-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/javax-websocket-client-impl-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/guice-servlet-4.2.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/jetty-jndi-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/snakeyaml-2.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/asm-commons-9.6.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/jersey-guice-1.19.4.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.12.7.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.12.7.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/guice-4.2.3.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/jsonschema2pojo-core-1.0.2.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/codemodel-2.6.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/jetty-annotations-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/websocket-client-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/bcutil-jdk15on-1.70.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/lib/javax.websocket-client-api-1.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-common-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-common-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-services-api-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-router-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-services-core-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-registry-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-globalpolicygenerator-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-api-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-client-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-applications-mawo-core-3.4.0.jar:/usr/local/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.4.0.jar\n",
            "STARTUP_MSG:   build = git@github.com:apache/hadoop.git -r bd8b77f398f626bb7791783192ee7a5dfaeec760; compiled by 'root' on 2024-03-04T06:35Z\n",
            "STARTUP_MSG:   java = 11.0.22\n",
            "************************************************************/\n",
            "2024-04-07 20:41:51,901 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
            "2024-04-07 20:41:52,124 INFO namenode.NameNode: createNameNode [-format]\n",
            "2024-04-07 20:41:53,352 INFO namenode.NameNode: Formatting using clusterid: CID-87197926-3c18-4cec-9ea7-adec842ba617\n",
            "2024-04-07 20:41:53,411 INFO namenode.FSEditLog: Edit logging is async:true\n",
            "2024-04-07 20:41:53,478 INFO namenode.FSNamesystem: KeyProvider: null\n",
            "2024-04-07 20:41:53,482 INFO namenode.FSNamesystem: fsLock is fair: true\n",
            "2024-04-07 20:41:53,482 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
            "2024-04-07 20:41:53,536 INFO namenode.FSNamesystem: fsOwner                = root (auth:SIMPLE)\n",
            "2024-04-07 20:41:53,536 INFO namenode.FSNamesystem: supergroup             = supergroup\n",
            "2024-04-07 20:41:53,536 INFO namenode.FSNamesystem: isPermissionEnabled    = true\n",
            "2024-04-07 20:41:53,536 INFO namenode.FSNamesystem: isStoragePolicyEnabled = true\n",
            "2024-04-07 20:41:53,537 INFO namenode.FSNamesystem: HA Enabled: false\n",
            "2024-04-07 20:41:53,618 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
            "2024-04-07 20:41:53,981 INFO blockmanagement.DatanodeManager: Slow peers collection thread shutdown\n",
            "2024-04-07 20:41:54,014 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit : configured=1000, counted=60, effected=1000\n",
            "2024-04-07 20:41:54,015 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
            "2024-04-07 20:41:54,022 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
            "2024-04-07 20:41:54,027 INFO blockmanagement.BlockManager: The block deletion will start around 2024 Apr 07 20:41:54\n",
            "2024-04-07 20:41:54,030 INFO util.GSet: Computing capacity for map BlocksMap\n",
            "2024-04-07 20:41:54,031 INFO util.GSet: VM type       = 64-bit\n",
            "2024-04-07 20:41:54,034 INFO util.GSet: 2.0% max memory 3.2 GB = 64.9 MB\n",
            "2024-04-07 20:41:54,034 INFO util.GSet: capacity      = 2^23 = 8388608 entries\n",
            "2024-04-07 20:41:54,135 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\n",
            "2024-04-07 20:41:54,135 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\n",
            "2024-04-07 20:41:54,147 INFO blockmanagement.BlockManagerSafeMode: Using 1000 as SafeModeMonitor Interval\n",
            "2024-04-07 20:41:54,147 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.999\n",
            "2024-04-07 20:41:54,147 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
            "2024-04-07 20:41:54,147 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
            "2024-04-07 20:41:54,148 INFO blockmanagement.BlockManager: defaultReplication         = 3\n",
            "2024-04-07 20:41:54,148 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
            "2024-04-07 20:41:54,149 INFO blockmanagement.BlockManager: minReplication             = 1\n",
            "2024-04-07 20:41:54,149 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
            "2024-04-07 20:41:54,149 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\n",
            "2024-04-07 20:41:54,149 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
            "2024-04-07 20:41:54,149 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
            "2024-04-07 20:41:54,252 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\n",
            "2024-04-07 20:41:54,253 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\n",
            "2024-04-07 20:41:54,253 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\n",
            "2024-04-07 20:41:54,253 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\n",
            "2024-04-07 20:41:54,281 INFO util.GSet: Computing capacity for map INodeMap\n",
            "2024-04-07 20:41:54,281 INFO util.GSet: VM type       = 64-bit\n",
            "2024-04-07 20:41:54,282 INFO util.GSet: 1.0% max memory 3.2 GB = 32.5 MB\n",
            "2024-04-07 20:41:54,282 INFO util.GSet: capacity      = 2^22 = 4194304 entries\n",
            "2024-04-07 20:41:54,302 INFO namenode.FSDirectory: ACLs enabled? true\n",
            "2024-04-07 20:41:54,302 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\n",
            "2024-04-07 20:41:54,302 INFO namenode.FSDirectory: XAttrs enabled? true\n",
            "2024-04-07 20:41:54,302 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
            "2024-04-07 20:41:54,332 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotFSLimit: 65536, maxSnapshotLimit: 65536\n",
            "2024-04-07 20:41:54,336 INFO snapshot.SnapshotManager: dfs.namenode.snapshot.deletion.ordered = false\n",
            "2024-04-07 20:41:54,346 INFO snapshot.SnapshotManager: SkipList is disabled\n",
            "2024-04-07 20:41:54,368 INFO util.GSet: Computing capacity for map cachedBlocks\n",
            "2024-04-07 20:41:54,368 INFO util.GSet: VM type       = 64-bit\n",
            "2024-04-07 20:41:54,369 INFO util.GSet: 0.25% max memory 3.2 GB = 8.1 MB\n",
            "2024-04-07 20:41:54,369 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
            "2024-04-07 20:41:54,414 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
            "2024-04-07 20:41:54,414 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
            "2024-04-07 20:41:54,414 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
            "2024-04-07 20:41:54,433 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
            "2024-04-07 20:41:54,434 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
            "2024-04-07 20:41:54,444 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
            "2024-04-07 20:41:54,444 INFO util.GSet: VM type       = 64-bit\n",
            "2024-04-07 20:41:54,446 INFO util.GSet: 0.029999999329447746% max memory 3.2 GB = 997.2 KB\n",
            "2024-04-07 20:41:54,446 INFO util.GSet: capacity      = 2^17 = 131072 entries\n",
            "2024-04-07 20:41:54,521 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1332397094-172.28.0.12-1712522514509\n",
            "2024-04-07 20:41:54,596 INFO common.Storage: Storage directory /tmp/hadoop-root/dfs/name has been successfully formatted.\n",
            "2024-04-07 20:41:54,709 INFO namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression\n",
            "2024-04-07 20:41:54,969 INFO namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 396 bytes saved in 0 seconds .\n",
            "2024-04-07 20:41:55,045 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
            "2024-04-07 20:41:55,073 INFO blockmanagement.DatanodeManager: Slow peers collection thread shutdown\n",
            "2024-04-07 20:41:55,240 INFO namenode.FSNamesystem: Stopping services started for active state\n",
            "2024-04-07 20:41:55,257 INFO namenode.FSNamesystem: Stopping services started for standby state\n",
            "2024-04-07 20:41:55,264 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\n",
            "2024-04-07 20:41:55,268 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
            "/************************************************************\n",
            "SHUTDOWN_MSG: Shutting down NameNode at 606cd77ca520/172.28.0.12\n",
            "************************************************************/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step II: Verifying Hadoop dfs"
      ],
      "metadata": {
        "id": "b7dNs50tQgse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add following to /usr/local/hadoop-3.4.0/etc/hadoop/hadoop-env.sh\n",
        "\n",
        "export HDFS_NAMENODE_USER=\"root\"\n",
        "\n",
        "export HDFS_DATANODE_USER=\"root\"\n",
        "\n",
        "export HDFS_SECONDARYNAMENODE_USER=\"root\"\n",
        "\n",
        "export YARN_RESOURCEMANAGER_USER=\"root\"\n",
        "\n",
        "export YARN_NODEMANAGER_USER=\"root\""
      ],
      "metadata": {
        "id": "Gc7_MYDrTLHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install -y openssh-server"
      ],
      "metadata": {
        "id": "b8M4-QFgUQKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo /etc/init.d/ssh start\n",
        "!service ssh status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKqwSC7_UUcG",
        "outputId": "a715cccb-6e8a-4651-d14e-21ae7fd5cc26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Starting OpenBSD Secure Shell server sshd\n",
            "   ...done.\n",
            " * sshd is running\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ssh-keygen -t rsa -b 4096 -C \"namitakalra@google.com\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zh7YiZk8UcMG",
        "outputId": "04cf6394-e788-494e-e95d-d54f98abd305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating public/private rsa key pair.\n",
            "Enter file in which to save the key (/root/.ssh/id_rsa): \n",
            "Created directory '/root/.ssh'.\n",
            "Enter passphrase (empty for no passphrase): \n",
            "Enter same passphrase again: \n",
            "Your identification has been saved in /root/.ssh/id_rsa\n",
            "Your public key has been saved in /root/.ssh/id_rsa.pub\n",
            "The key fingerprint is:\n",
            "SHA256:CCcxuvvownX6R9N8SkoZj7RQ1RsEEa272T4QD5a0sUo namitakalra@google.com\n",
            "The key's randomart image is:\n",
            "+---[RSA 4096]----+\n",
            "|    o   .=B.     |\n",
            "|   . o . o +     |\n",
            "|  . o o . * o    |\n",
            "|   . = E O .     |\n",
            "|  .   = S =      |\n",
            "|   o . O B o     |\n",
            "|. o o o + B      |\n",
            "|.. +   o + o     |\n",
            "| oo o..   ...    |\n",
            "+----[SHA256]-----+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /root/.ssh/id_rsa.pub > /root/.ssh/authorized_keys"
      ],
      "metadata": {
        "id": "zV616D-UUkU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ssh-keyscan -H localhost >> /root/.ssh/known_hosts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSD4-TUEUo6A",
        "outputId": "8e73391f-536b-4b42-becb-31db01e089eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# localhost:22 SSH-2.0-OpenSSH_8.9p1 Ubuntu-3ubuntu0.6\n",
            "# localhost:22 SSH-2.0-OpenSSH_8.9p1 Ubuntu-3ubuntu0.6\n",
            "# localhost:22 SSH-2.0-OpenSSH_8.9p1 Ubuntu-3ubuntu0.6\n",
            "# localhost:22 SSH-2.0-OpenSSH_8.9p1 Ubuntu-3ubuntu0.6\n",
            "# localhost:22 SSH-2.0-OpenSSH_8.9p1 Ubuntu-3ubuntu0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 700 /root/.ssh\n",
        "!chmod 600 /root/.ssh/authorized_keys"
      ],
      "metadata": {
        "id": "8PtTad76UmhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.4.0/sbin/start-dfs.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJ8w8-iDQk7K",
        "outputId": "f4851161-6d22-422e-dcab-c03df9fe757f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting namenodes on [606cd77ca520]\n",
            "606cd77ca520: Warning: Permanently added '606cd77ca520' (ED25519) to the list of known hosts.\r\n",
            "Starting datanodes\n",
            "Starting secondary namenodes [606cd77ca520]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step III: Verifying Yarn Script"
      ],
      "metadata": {
        "id": "IxtOKT-fQ1zU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.4.0/sbin/start-yarn.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGNaFV60Q51G",
        "outputId": "31d33821-bac8-448a-e953-b11b34a7a840"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting resourcemanager\n",
            "Starting nodemanagers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step IV: Accessing Hadoop on Browser\n",
        "\n",
        "The default port number to access all applications of cluster is 8088. Use the following url to visit this service.\n",
        "\n",
        "http://localhost:8088/"
      ],
      "metadata": {
        "id": "9c67ZT3MQ9YU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using `ngrok` to create a public url"
      ],
      "metadata": {
        "id": "dAfQMi1tV38c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43ZMO0BwRFjj",
        "outputId": "f878d813-a91f-485f-fe52-a2368dd90ce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-07 20:50:06--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 54.161.241.46, 18.205.222.128, 52.202.168.65, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|54.161.241.46|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13921656 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.28M  17.6MB/s    in 0.8s    \n",
            "\n",
            "2024-04-07 20:50:08 (17.6 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13921656/13921656]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcovbtXlRevB",
        "outputId": "9dc73b43-597a-402e-cbe4-40a8097b279e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r ngrok /usr/local/bin"
      ],
      "metadata": {
        "id": "BSYLZ2qCRg9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken \"2e66TXIkQSNBNkqYO2lPE6FvuxU_wLQjMY5yA9DGEjzgjDU6\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_3V4giJRi0V",
        "outputId": "f083dd99-eeb8-450a-cc3b-e249e22f32d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_O1JeTMmU5oq",
        "outputId": "3d976950-cf8a-4bf5-8fcf-a996f9b5977f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.1.6-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok"
      ],
      "metadata": {
        "id": "xtUXTNEbU9a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.set_auth_token(\"2emwk6Enp8UGLQEyCuMibLppkdE_bvVQypkDmjuhkewsSQ4X\")\n",
        "\n",
        "tunnel = ngrok.connect(addr=\"8088\", proto=\"http\")\n",
        "# The public URL is directly available from the tunnel object.\n",
        "public_url = tunnel.public_url\n",
        "print(\"Tunnel Public URL:\", public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p95bTzEZRGLM",
        "outputId": "ef554010-2dbd-4aab-8bb5-3c9c5cba47a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2024-04-07T20:53:41+0000 lvl=warn msg=\"ngrok config file found at both XDG and legacy locations, using XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tunnel Public URL: https://94d8-34-83-145-234.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.4.0/bin/hadoop jar /usr/local/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.4.0.jar grep ~/input ~/grep_example 'allowed*'"
      ],
      "metadata": {
        "id": "1ZIQBA1UNZl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat ~/grep_example/*"
      ],
      "metadata": {
        "id": "kdU3pv7PNlFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat ~/input/* | grep -c \"allowed\""
      ],
      "metadata": {
        "id": "DhOdk41gNwXV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}